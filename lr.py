# -*- coding: utf-8 -*-
"""Untitled2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1xxK8qXqDv95weS-QRXcGe_AJj3wrPEAX
"""

import scipy.io
import numpy as np

class LogisticRegression:
    # constructor to take learning rate and number of iterations as parameters
    def __init__(self, lr=0.001, epoch=10000):
        self.lr = lr
        self.epoch = epoch
    
    # data normalization
    def normalize(self, X):
        X = (X - self.x_mean) / self.x_stddev
        return X
    
    def predict(self, X):
        X = self.normalize(X)
        # matrix of W(i)X(i)
        linear = self._linear(X)
        
        # predictions over the sigmoid function
        preds = self._non_linear(linear)
        
        # returning 1 or 0
        return (preds >= 0.5).astype('int')

     # sigmoid function, calculating the non-linear score
    def _non_linear(self, X):
        return 1 / (1 + np.exp(-X))
     
     # calculating the linear score
    def _linear(self, X):
        return np.dot(X, self.weights) + self.bias
     
    
    def initialize_weights(self, X):
        # setting weight as a 2*1 matrix with random values
        self.weights = np.random.rand(X.shape[1], 1)
        # setting bias = 0 for all the features initially for prediction
        self.bias = np.zeros((1,))

    def fit(self, X_train, Y_train):
      
        self.initialize_weights(X_train)

        # get mean and stddev for normalization
        self.x_mean = X_train.mean(axis=0).T
        self.x_stddev = X_train.std(axis=0).T

        # normalize data
        X_train = self.normalize(X_train)
            
        # Run gradient descent for n iterations
        for i in range(self.epoch):
            # make normalized predictions
            probs = self._non_linear(self._linear(X_train))
            diff = probs - Y_train
            
            # calculating the gradient
            delta_w = np.matmul(np.transpose(X_train),diff)
            delta_b = np.mean(diff)
            
            # update weights
            self.weights = self.weights - np.dot(self.lr,delta_w)
            self.bias = self.bias - np.dot(self.lr,delta_b)
        return self

    # calculating accuracy
    def accuracy(self, X, y):
        preds = self.predict(X)     
        return np.mean(preds == y)
   
    #calculating cross entropy
    def loss(self, X, y):
        probs = self._non_linear(self._linear(X))

        # entropy when true class is positive
        pos_log = y * np.log(probs + 1e-15)
        # entropy when true class is negative
        neg_log = (1 - y) * np.log((1 - probs) + 1e-15)
        #mean of entropy
        l = -np.mean(pos_log + neg_log)
        return l


def data_Extraction():

    Numpyfile= scipy.io.loadmat('/content/mnist_data.mat')
    trX = Numpyfile['trX']
    trY = Numpyfile['trY']
    tsX = Numpyfile['tsX']
    tsY = Numpyfile['tsY']
    return trX,trY,tsX,tsY
  
  
def feature_Extraction(X):
    dataset = []
    for i in range(len(X)):
        temp = np.array([np.mean(X[i]), np.std(X[i])])
        dataset.append(temp)
    dataset = np.array(dataset, dtype=np.float128)
    return dataset
  
    
#if __name__ == "__main__": 
def main(): 
    print("Logistic Regression")
    #data Extraction
    trX,trY,tsX,tsY = data_Extraction()
    
    #data transformation
    trY = np.transpose(trY)
    tsY = np.transpose(tsY)
    
    
    #feature extraction
    
    #training dataset
    dataset = feature_Extraction(trX)
    
    #testing dataset
    datatest = feature_Extraction(tsX)

    lr = LogisticRegression()
    lr.fit(dataset, trY)
    print()
    print('Loss on Testing Data: {:.2f}'.format(lr.loss(datatest, tsY)))
    print('Overall Accuracy: {:.2f}%'.format(lr.accuracy(datatest, tsY) * 100))
    print('*********************************')
